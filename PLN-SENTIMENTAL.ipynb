{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 70ms/step - loss: 1.5118 - accuracy: 0.4626 - val_loss: 1.3607 - val_accuracy: 0.5070\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.3356 - accuracy: 0.5089 - val_loss: 1.2129 - val_accuracy: 0.5070\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.2171 - accuracy: 0.5089 - val_loss: 1.1327 - val_accuracy: 0.5070\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 1.0622 - accuracy: 0.5374 - val_loss: 0.8895 - val_accuracy: 0.6620\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.8226 - accuracy: 0.6406 - val_loss: 0.7795 - val_accuracy: 0.6620\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.7286 - accuracy: 0.6833 - val_loss: 0.7916 - val_accuracy: 0.7183\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.6991 - accuracy: 0.6975 - val_loss: 0.7879 - val_accuracy: 0.7183\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.6549 - accuracy: 0.7011 - val_loss: 0.7743 - val_accuracy: 0.7183\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.6122 - accuracy: 0.7153 - val_loss: 0.7232 - val_accuracy: 0.7183\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.5741 - accuracy: 0.7438 - val_loss: 0.6915 - val_accuracy: 0.7465\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6367 - accuracy: 0.7727\n",
      "Test Score: 0.6367136240005493\n",
      "Test Accuracy: 0.7727272510528564\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Carrega o léxico em um DataFrame\n",
    "lexico = pd.read_csv('lexico_czar.csv')\n",
    "\n",
    "# Remove stopwords e pontuações\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "lexico['text'] = lexico['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "lexico['text'] = lexico['text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "\n",
    "# Mapeia as emoções do léxico para os rótulos utilizados no modelo\n",
    "emotions_mapping = {'Felicidade': 'happy', 'Tristeza': 'sad', 'Raiva': 'angry', 'Neutralidade': 'neutral', 'Sarcasmo': 'sarcasm'}\n",
    "lexico['emotion'] = lexico['emotion'].map(emotions_mapping)\n",
    "\n",
    "# Define os dados de entrada e saída\n",
    "X = lexico['text'].values\n",
    "Y = pd.get_dummies(lexico['emotion']).values\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Define o tokenizer e o tamanho máximo das sequências\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "max_length = len(max(X_train, key=len).split())\n",
    "\n",
    "# Transforma os textos em sequências de tokens e preenche as sequências com zeros (padding)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding='post')\n",
    "\n",
    "# Define o modelo da rede neural\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treina o modelo com o conjunto de treino e valida com o conjunto de teste\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "history = model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Avalia o modelo com o conjunto de teste\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
